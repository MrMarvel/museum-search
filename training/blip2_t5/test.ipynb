{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vision_text_dual_encoder import VisionTextDualEncoderConfig, VisionTextDualEncoderModel\n",
    "from transformers import (\n",
    "    AutoImageProcessor,\n",
    "    AutoModel,\n",
    "    AutoTokenizer,\n",
    "    HfArgumentParser,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    set_seed,\n",
    ")\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'blip2_t5_finetuned'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "model = VisionTextDualEncoderModel.from_pretrained(\n",
    "    model_path,\n",
    "    config=VisionTextDualEncoderConfig.from_pretrained(model_path),\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map='cuda'\n",
    ").eval()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_path,\n",
    "    use_fast=False,\n",
    ")\n",
    "\n",
    "image_processor = AutoImageProcessor.from_pretrained(\n",
    "    model_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "img_url = 'https://sun1-92.userapi.com/impg/fnn1LcU9RbFGAk8Y64_WCB0nrAzjJKFa-5EpZg/STS6yLrU-XI.jpg?size=640x480&quality=95&sign=dab78f730e79022d36a499e0ae1783be&type=album'\n",
    "image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\n",
    "\n",
    "text_inputs = tokenizer([\"A man is walking with a dog\", \"A man write in notebook\"], return_tensors=\"pt\", padding=True)\n",
    "image_inputs = image_processor(image, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    out = model(**image_inputs.to('cuda'), **text_inputs.to('cuda'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3351/767894517.py:1: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  torch.nn.functional.softmax(out.logits_per_image)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.5234, 0.4766]], device='cuda:0', dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.functional.softmax(out.logits_per_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1621],\n",
       "        [0.2207]], device='cuda:0', dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.text_embeds @ out.image_embeds.T"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
